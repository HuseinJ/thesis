\chapter{Implementation}\label{chapter:implementation}
This chapter documents the empirical investigation of LLM-assisted bounded context extraction, conducted through a systematic comparison of AI-generated domain models against manually-crafted architectures within FTAPI's software ecosystem.

\Section{LLM Selection and Configuration}
The initial phase focused on evaluating different LLM options for domain modeling tasks. Given the computational resource constraints typical in academic research environments, deploying and running effective open-source models locally proved impractical. Consequently, the evaluation was conducted using commercially available AI chat interfaces, which provided access to state-of-the-art models without requiring extensive computational infrastructure.

\subsection{Initial Model Evaluation}
Three leading LLM platforms were selected for preliminary testing based on their documented capabilities in code analysis and architectural reasoning tasks: Claude Opus 4.1 (Anthropic), GPT-5 (OpenAI), and Gemini 2.5 Pro (Google). This selection represented the current state-of-the-art in commercially available large language models, each offering distinct approaches to natural language understanding and reasoning.

\subsection{Resource Constraints and Practical Considerations}
The decision to utilize commercial chat interfaces rather than self-hosted open-source alternatives was driven by practical limitations. While open-source models such as LLaMA 2 and CodeLlama were initially considered, the computational requirements for running these models effectively exceeded available hardware resources. The commercial platforms provided consistent access to powerful models without the overhead of infrastructure management, enabling focus on the core research questions rather than technical deployment challenges.

\section{Prompt Engineering Framework}
A robust and consistent prompt engineering framework was central to this research, designed to elicit sophisticated architectural reasoning from Large Language Models (LLMs). This section details the rationale for the multi-model evaluation approach, the overall design philosophy of the prompts, and the specific role-based architecture employed throughout the experiments.

\subsection{Rationale for a Multi-Model Approach}
The performance differences observed across LLMs exhibit a strong dependency on the specific prompt engineering approach employed. The varying characteristics of each model can be attributed not only to their inherent architectural capabilities but also to the complex interaction between their training data and the particular prompt formulations developed for this research. A prompt strategy that is effective for one model may yield suboptimal results with another, suggesting that effective LLM-assisted domain modeling requires careful consideration of this prompt-model compatibility.

Given this uncertainty, committing to a single LLM early in the research process would have introduced significant bias. Instead, a comprehensive multi-model evaluation was adopted. All evaluated LLMs were tested throughout the experimental phase, allowing for a direct empirical comparison of their strengths and weaknesses. This strategy proved invaluable for identifying model-specific behaviors that would not have been apparent from theoretical analysis alone. By maintaining flexibility, the research could adapt to empirical findings rather than being constrained by initial assumptions about a single model's suitability for DDD-based architectural analysis.

\subsection{Prompt Development Strategy}
To ensure a fair and rigorous comparison across models, the prompt engineering approach was designed to simulate the experience of collaborating with a senior Domain-Driven Design (DDD) specialist. Rather than simply requesting architectural outputs, the prompts were structured to create an interactive, questioning-based methodology that mirrors real-world DDD consulting practices. This approach aimed to leverage the LLM's reasoning capabilities by embedding it within a realistic professional context where architectural decisions must be justified and explored thoroughly.

\subsection{Role-Based Prompt Architecture}
The core of the framework is a comprehensive role-based architecture that positions the LLM as an expert consultant. This strategy moves beyond simple instructions to create a professional persona embodying the critical thinking essential for architectural analysis.

\subsubsection{Core Role Definition}
The primary role prompt (see Appendix~\ref{app:role-prompt}) establishes the LLM as a "Senior Domain-Driven Design Specialist \& Architectural Sparring Partner" with over 10 years of enterprise DDD implementation experience. This detailed persona specification serves multiple strategic purposes: it provides contextual grounding for the expected level of architectural sophistication, establishes an interactive rather than passive analytical approach, and creates behavioral expectations for rigorous questioning and assumption challenging.

\subsubsection{Behavioral Guidelines and Context Simulation}
The role definition includes specific behavioral instructions that guide the LLM's analytical process. It is instructed to enforce DDD best practices, engage in collaborative modeling, and actively challenge vague or ambiguous concepts. This behavioral framework ensures consistency in how the model approaches domain modeling tasks across different requirement sets.

By embedding the LLM within this realistic consulting context—complete with "red flags" that trigger intervention, a communication style based on Socratic questioning, and an emphasis on business value—the prompts activate more sophisticated reasoning patterns. This comprehensive context simulation was highly effective in generating nuanced architectural insights that reflected genuine domain expertise. The role-based architecture proved essential for maintaining consistency across the multi-phase analysis, ensuring each step built upon the established expert persona.

\subsection{Structured Analysis Workflow}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance=0.8cm,
        phase/.style={
            rectangle, rounded corners=4pt, minimum width=7.2cm,
            minimum height=1.4cm, text centered, draw=black, font=\small\bfseries, text width=7cm
        },
        note/.style={font=\scriptsize\itshape},
        arrow/.style={thick,->,>=stealth, color=blue!70},
        feedback/.style={thick,->,>=stealth, dashed, color=gray!60}
    ]
    
    % Title
    \node[font=\Large\bfseries] at (0, 1.2) {LLM-Assisted DDD Analysis Workflow};
    
    % Phases
    \node (phase1) [phase, fill=blue!10] at (0, 0) {
        Phase 1: Ubiquitous Language Establishment\\
        \scriptsize Extract domain vocabulary, define glossary
    };
    
    \node (phase2) [phase, below=of phase1, fill=red!10] {
        Phase 2: Event Storming Simulation\\
        \scriptsize Identify events, commands, actors
    };
    
    \node (phase3) [phase, below=of phase2, fill=orange!10] {
        Phase 3: Bounded Context Identification\\
        \scriptsize Group concepts into cohesive contexts
    };
    
    \node (phase4) [phase, below=of phase3, fill=purple!10] {
        Phase 4: Aggregate Design\\
        \scriptsize Define aggregates, entities, invariants
    };
    
    \node (phase5) [phase, below=of phase4, fill=green!10] {
        Phase 5: Technical Architecture Mapping\\
        \scriptsize Design ports, adapters, infrastructure
    };
    
    \node (output) [phase, below=of phase5, fill=gray!10, minimum height=1.5cm, font=\bfseries] {
        Complete Domain Model\\
        \scriptsize Bounded Contexts • Aggregates • Architecture
    };
    
    % Arrows between phases
    \draw [arrow] (phase1.south) -- (phase2.north);
    \draw [arrow] (phase2.south) -- (phase3.north);
    \draw [arrow] (phase3.south) -- (phase4.north);
    \draw [arrow] (phase4.south) -- (phase5.north);
    \draw [arrow] (phase5.south) -- (output.north);
    
    % Side Labels Aligned to Full Diagram Height
\node[rotate=0, anchor=center, font=\scriptsize, gray] at (-6.5, 0.0) {Requirements Input};

% Dashed Input Arrow pointing to Phase 1
\draw[->, thick, dashed, gray!70] (-5.0, 0.0) -- (phase1.west);
    
    \end{tikzpicture}
    \caption{Refined Five-Phase LLM-Assisted DDD Analysis Workflow}
    \label{fig:refined-ddd-workflow}
    \end{figure}

The prompt framework implements a five-phase analysis workflow, each designed to build upon previous insights while maintaining focus on specific Domain-Driven Design (DDD) aspects. As illustrated in Figure~\ref{fig:refined-ddd-workflow}, this process follows a structured progression from vocabulary definition to technical architecture mapping, culminating in a complete domain model that reflects both business requirements and architectural clarity.

\subsubsection{Phase 1: Ubiquitous Language Establishment}
The initial phase systematically extracts and defines the core domain vocabulary from requirement specifications through a structured glossary approach (see Appendix~\ref{app:ubiquitous-language-prompt}). This foundational step ensures all subsequent analysis operates within a consistent linguistic framework, identifying key business terms, their definitions, contextual usage, and potential ambiguities. The prompt guides the LLM through comprehensive analysis of nouns, verbs, and business concepts while emphasizing business-focused rather than technical definitions. The structured table format captures term definitions, business context, related concepts, and clarification needs, establishing the vocabulary foundation for all architectural decisions.

\subsubsection{Phase 2: Event Storming Simulation}
Building directly upon the established vocabulary, this phase identifies the temporal flow and dynamic behaviors within the system (see Appendix~\ref{app:event-storming-prompt}). The prompt guides the LLM through systematic identification of domain events in chronological order, mapping each event to its triggering commands, responsible actors, applicable policies, and handling aggregates. This phase transforms static vocabulary into dynamic process understanding, revealing the business workflows and state transitions that drive architectural requirements.

\subsubsection{Phase 3: Bounded Context Identification}
The bounded context mapping phase leverages the established vocabulary and process understanding to identify natural boundaries within the domain (see Appendix~\ref{app:bounded-context-prompt}). The prompt directs the LLM to group related terms from the glossary into cohesive contexts, defining each context's core purpose, key aggregates, and context-specific language variations. This phase establishes the high-level architectural boundaries that will guide detailed design decisions.

\subsubsection{Phase 4: Aggregate Design}
Within each identified bounded context, this phase focuses on detailed structural design ensuring proper encapsulation and consistency management (see Appendix~\ref{app:aggregate-design-prompt}). The prompt guides the LLM through identification of aggregate roots, definition of consistency boundaries, specification of contained entities and value objects, and articulation of business invariants. This phase translates conceptual boundaries into concrete structural components.

\subsubsection{Phase 5: Technical Architecture Mapping}
The technical architecture phase translates domain insights into implementable architectural patterns following hexagonal architecture principles (see Appendix~\ref{app:technical-architecture-prompt}). This phase ensures clean separation between domain logic and technical infrastructure while maintaining traceability to business requirements.

\subsubsection{Workflow Integration and Dependencies}
Each phase explicitly builds upon the outputs of the previous ones, creating a cohesive analytical progression from vocabulary definition to detailed implementation guidance. Importantly, each phase is approached as an iterative dialogue with the LLM, allowing for continuous refinement through interactive questioning and clarification until a satisfactory result is achieved. This structured yet flexible process helps prevent common issues such as premature technical decisions or incomplete domain understanding, while ensuring thorough coverage of all relevant DDD architectural concerns.

\section{Requirements Gathering and Preparation}

\subsection{Source Documentation Analysis}
Requirements for both SecuRooms and SecuMails domains were systematically extracted from FTAPI's existing product documentation. This approach ensured that the LLM analysis would be based on the same foundational information used in the development processes.

\subsection{Requirements Compilation Strategy}
SecuRooms represents a smaller domain compared to SecuMails, but provides an ideal validation case since it has already been successfully modularized using manual DDD practices. This existing architecture serves as the benchmark against which LLM-generated models can be evaluated. The SecuMails domain requirements represent the primary target for architectural modernization, encompassing the core email functionality that remains in monolithic form.

\subsection{Input Preparation Strategy}
For both domains, all gathered requirements were consolidated into structured text documents optimized for LLM processing. Requirements were formatted as plain text documents, with each domain's requirements organized in a single file ready for direct input into the AI system. This approach enabled seamless progression through the five-phase analysis workflow while allowing the LLM to maintain context across all phases and build progressively more detailed architectural insights

\section{Architecture Generation Process}
\subsection{LLM-Assisted Domain Model Creation}
Using the prepared requirements documents, both SecuRooms and SecuMails domains were systematically processed through the established five-phase workflow. The analysis generated comprehensive architectural candidates for each domain, with multiple iterations performed where necessary to refine and clarify the resulting domain models.

\subsection{Architecture Candidate Documentation}
The LLM-generated outputs from each phase were systematically captured and consolidated into structured architectural candidates. These candidates included complete bounded context definitions, aggregate specifications, entity relationships, and technical architecture mappings. For SecuRooms, this process produced architectural proposals that could be directly compared against the existing manually-designed implementation.

\subsection{Output Validation and Refinement}
Each generated architecture candidate underwent internal validation to ensure completeness and internal consistency. Where ambiguities or gaps were identified in the initial outputs, additional iterations through relevant workflow phases were conducted to achieve satisfactory architectural coverage.

\section{Expert Evaluation Preparation}

\subsection{Interview Design and Structure}
The expert evaluation was designed around the core research questions. The interview structure incorporated qualitative feedback to capture comprehensive insights.

Given that SecuRooms already has a manually-designed DDD implementation, the interviews were structured to evaluate both domains differently. For SecuRooms, experts could directly compare LLM outputs against the existing proven architecture. For SecuMails, experts assessed the LLM proposals on their own merits as potential modernization strategies.

\subsubsection{Interview Phases and Core Questions}
\label{sec:interview_phases}

The interview was structured into four parts and the following questions were prepared for the interview to guide throughout

\paragraph{Part 1: Introduction and Goal Alignment (5 minutes)}
The objective of this phase was to brief the expert on the purpose of the study.
\begin{itemize}
    \item \textbf{Introduction:} A brief overview of the thesis goal: to evaluate the effectiveness of LLMs in identifying bounded contexts from complex enterprise requirements.
    \item \textbf{Context:} Explanation of the two cases: SecuRooms as a validation case against a known benchmark, and SecuMails as an exploratory case for a monolithic modernization challenge.
    \item \textbf{Task:} Clarification that the expert's role is to critique the AI-generated models based on their deep domain knowledge and experience with Domain-Driven Design.
\end{itemize}

\paragraph{Part 2: Comparative Evaluation of SecuRooms (20 minutes)}
This part focused on directly comparing the LLM-generated model against the existing, human-designed architecture for SecuRooms.

\begin{itemize}
    \item \textbf{Qualitative Probing Questions:}
    \begin{itemize}
        \item "Does the extracted Ubiquitous language represent the real language used for securooms"
        \item "Do the extracted events represent all the events what in the Securooms domain happen? Do you mis anything here ?"
        \item "Did the LLM identify any alternative groupings or potential improvements that we missed during the manual design? Conversely, what critical elements did it completely omit?"
        \item "Do you think the extracted Aggregates represent the real core aggregates we currently have?"
    \end{itemize}
\end{itemize}

\paragraph{Phase 3: Standalone Evaluation of SecuMails (20 minutes)}
This part was conceptualized to assess the LLM-generated architecture for SecuMails on its own merits as a viable modernization strategy.

\begin{itemize}
    \item \textbf{Qualitative Probing Questions:}
    \begin{itemize}
        \item "Based on your understanding of the SecuMails monolithic challenges, does this AI-proposed architecture represent a plausible and effective path forward? Why or why not?"
        \item "If you were tasked with modernizing SecuMails, would you consider this LLM output a useful starting point? What would you change, and what would you keep?"
        \item "Would this proposal be helpful to you as the architect who is tasked with defining a modernized architecture"
    \end{itemize}
\end{itemize}

\paragraph{Phase 4: Overall Impressions and Conclusion (10 minutes)}
This final part should capture the experts' holistic views on the practical implications of this technology.

\begin{itemize}
    \item \textbf{Discussion Questions:}
    \begin{itemize}
        \item "Overall, how would you describe the utility of the LLM as an 'architectural sparring partner' in the domain modeling process?"
        \item "To what extent could this approach accelerate or improve the quality of architectural design at FTAPI, especially considering constraints like tight deadlines and business pressure?"
        \item "What are the most significant limitations or risks you foresee in relying on LLMs for these critical design tasks?"
        \item "Do you have any final recommendations for how this methodology could be improved or applied in the future?"
    \end{itemize}
\end{itemize}

\subsection{Expert Evaluation Execution}
Following the prepared design, a series of semi-structured interviews were conducted with 3 experts from FTAPI Software GmbH. The participants included one tech lead and two senior Developers, each with extensive experience in the company's domain and the principles of Domain-Driven Design. The interviews were conducted via video conference, lasted approximately 50min each, and were recorded and transcribed for analysis. The gathered qualitative data was then thematically analyzed to identify recurring patterns, points of consensus, and divergent opinions regarding the LLM-generated architectural models.