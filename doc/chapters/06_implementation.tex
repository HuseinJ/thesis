\chapter{Implementation}\label{chapter:implementation}
This chapter documents the empirical investigation of LLM-assisted bounded context extraction, conducted through a systematic comparison of AI-generated domain models against manually-crafted architectures within FTAPI's software ecosystem.

\section{LLM Selection and Configuration}
The initial phase focused on evaluating different LLM options for domain modeling tasks. Given the computational resource constraints typical in academic research environments, deploying and running effective open-source models locally proved impractical. Consequently, the evaluation was conducted using commercially available AI chat interfaces, which provided access to state-of-the-art models without requiring extensive computational infrastructure.

\subsection{Initial Model Evaluation}
Three leading LLM platforms were selected for preliminary testing based on their documented capabilities in code analysis and architectural reasoning tasks: Claude Opus 4.1 (Anthropic), GPT-5 (OpenAI), and Gemini 2.5 Pro (Google). This selection represented the current state-of-the-art in commercially available large language models, each offering distinct approaches to natural language understanding and reasoning.

\subsection{Resource Constraints and Practical Considerations}
The decision to utilize commercial chat interfaces rather than self-hosted open-source alternatives was driven by practical limitations. While open-source models such as LLaMA 2 and CodeLlama were initially considered, the computational requirements for running these models effectively exceeded available hardware resources. The commercial platforms provided consistent access to powerful models without the overhead of infrastructure management, enabling focus on the core research questions rather than technical deployment challenges.

\section{Prompt Engineering Framework}
A robust and consistent prompt engineering framework was central to this research, designed to elicit sophisticated architectural reasoning from Large Language Models (LLMs). This section details the rationale for the multi-model evaluation approach, the overall design philosophy of the prompts, and the specific role-based architecture employed throughout the experiments.

\subsection{Rationale for a Multi-Model Approach}
The performance differences observed across LLMs exhibit a strong dependency on the specific prompt engineering approach employed. The varying characteristics of each model can be attributed not only to their inherent architectural capabilities but also to the complex interaction between their training data and the particular prompt formulations developed for this research. A prompt strategy that is effective for one model may yield suboptimal results with another, suggesting that effective LLM-assisted domain modeling requires careful consideration of this prompt-model compatibility.

Given this uncertainty, committing to a single LLM early in the research process would have introduced significant bias. Instead, a comprehensive multi-model evaluation was adopted. All evaluated LLMs were tested throughout the experimental phase, allowing for a direct empirical comparison of their strengths and weaknesses. This strategy proved invaluable for identifying model-specific behaviors that would not have been apparent from theoretical analysis alone. By maintaining flexibility, the research could adapt to empirical findings rather than being constrained by initial assumptions about a single model's suitability for DDD-based architectural analysis.

\subsection{Prompt Development Strategy}
To ensure a fair and rigorous comparison across models, the prompt engineering approach was designed to simulate the experience of collaborating with a senior Domain-Driven Design (DDD) specialist. Rather than simply requesting architectural outputs, the prompts were structured to create an interactive, questioning-based methodology that mirrors real-world DDD consulting practices. This approach aimed to leverage the LLMs reasoning capabilities by embedding it within a realistic professional context where architectural decisions must be justified and explored thoroughly.

\subsection{Role-Based Prompt Architecture}
The core of the framework is a comprehensive role-based architecture that positions the LLM as an expert consultant. This strategy moves beyond simple instructions to create a professional persona embodying the critical thinking essential for architectural analysis.

\subsubsection{Core Role Definition}
The primary role prompt (see Appendix~\ref{app:role-prompt}) establishes the LLM as a "Senior Domain-Driven Design Specialist \& Architectural Sparring Partner" with over 10 years of enterprise DDD implementation experience. This detailed persona specification serves multiple strategic purposes: it provides contextual grounding for the expected level of architectural sophistication, establishes an interactive rather than passive analytical approach, and creates behavioral expectations for rigorous questioning and assumption challenging.

\subsubsection{Behavioral Guidelines and Context Simulation}
The role definition includes specific behavioral instructions that guide the LLMs analytical process. It is instructed to enforce DDD best practices, engage in collaborative modeling, and actively challenge vague or ambiguous concepts. This behavioral framework ensures consistency in how the model approaches domain modeling tasks across different requirement sets.

By embedding the LLM within this realistic consulting context—complete with "red flags" that trigger intervention, a communication style based on Socratic questioning, and an emphasis on business value—the prompts activate more sophisticated reasoning patterns. This comprehensive context simulation was highly effective in generating nuanced architectural insights that reflected genuine domain expertise. The role-based architecture proved essential for maintaining consistency across the multi-phase analysis, ensuring each step built upon the established expert persona.

\subsection{Structured Analysis Workflow}\label{section:workflow}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance=0.8cm,
        phase/.style={
            rectangle, rounded corners=4pt, minimum width=7.2cm,
            minimum height=1.4cm, text centered, draw=black, font=\small\bfseries, text width=7cm
        },
        note/.style={font=\scriptsize\itshape},
        arrow/.style={thick,->,>=stealth, color=blue!70},
        feedback/.style={thick,->,>=stealth, dashed, color=gray!60}
    ]
    
    % Title
    \node[font=\Large\bfseries] at (0, 1.2) {LLM-Assisted DDD Analysis Workflow};
    
    % Phases
    \node (phase1) [phase, fill=blue!10] at (0, 0) {
        Phase 1: Ubiquitous Language Establishment\\
        \scriptsize Extract domain vocabulary, define glossary
    };
    
    \node (phase2) [phase, below=of phase1, fill=red!10] {
        Phase 2: Event Storming Simulation\\
        \scriptsize Identify events, commands, actors
    };
    
    \node (phase3) [phase, below=of phase2, fill=orange!10] {
        Phase 3: Bounded Context Identification\\
        \scriptsize Group concepts into cohesive contexts
    };
    
    \node (phase4) [phase, below=of phase3, fill=purple!10] {
        Phase 4: Aggregate Design\\
        \scriptsize Define aggregates, entities, invariants
    };
    
    \node (phase5) [phase, below=of phase4, fill=green!10] {
        Phase 5: Technical Architecture Mapping\\
        \scriptsize Design ports, adapters, infrastructure
    };
    
    \node (output) [phase, below=of phase5, fill=gray!10, minimum height=1.5cm, font=\bfseries] {
        Complete Domain Model\\
        \scriptsize Bounded Contexts • Aggregates • Architecture
    };
    
    % Arrows between phases
    \draw [arrow] (phase1.south) -- (phase2.north);
    \draw [arrow] (phase2.south) -- (phase3.north);
    \draw [arrow] (phase3.south) -- (phase4.north);
    \draw [arrow] (phase4.south) -- (phase5.north);
    \draw [arrow] (phase5.south) -- (output.north);
    
    % Side Labels Aligned to Full Diagram Height
\node[rotate=0, anchor=center, font=\scriptsize, gray] at (-6.5, 0.0) {Requirements Input};

% Dashed Input Arrow pointing to Phase 1
\draw[->, thick, dashed, gray!70] (-5.0, 0.0) -- (phase1.west);
    
    \end{tikzpicture}
    \caption{Refined Five-Phase LLM-Assisted DDD Analysis Workflow}
    \label{fig:refined-ddd-workflow}
    \end{figure}

The prompt framework implements a five-phase analysis workflow, each designed to build upon previous insights while maintaining focus on specific Domain-Driven Design (DDD) aspects. As illustrated in Figure~\ref{fig:refined-ddd-workflow}, this process follows a structured progression from vocabulary definition to technical architecture mapping, culminating in a complete domain model that reflects both business requirements and architectural clarity.

\subsubsection{Phase 1: Ubiquitous Language Establishment}
The initial phase systematically extracts and defines the core domain vocabulary from requirement specifications through a structured glossary approach (see Appendix~\ref{app:ubiquitous-language-prompt}). This foundational step ensures all subsequent analysis operates within a consistent linguistic framework, identifying key business terms, their definitions, contextual usage, and potential ambiguities. The prompt guides the LLM through comprehensive analysis of nouns, verbs, and business concepts while emphasizing business-focused rather than technical definitions. The structured table format captures term definitions, business context, related concepts, and clarification needs, establishing the vocabulary foundation for all architectural decisions.

\subsubsection{Phase 2: Event Storming Simulation}
Building directly upon the established vocabulary, this phase identifies the temporal flow and dynamic behaviors within the system (see Appendix~\ref{app:event-storming-prompt}). The prompt guides the LLM through systematic identification of domain events in chronological order, mapping each event to its triggering commands, responsible actors, applicable policies, and handling aggregates. This phase transforms static vocabulary into dynamic process understanding, revealing the business workflows and state transitions that drive architectural requirements.

\subsubsection{Phase 3: Bounded Context Identification}
The bounded context mapping phase leverages the established vocabulary and process understanding to identify natural boundaries within the domain (see Appendix~\ref{app:bounded-context-prompt}). The prompt directs the LLM to group related terms from the glossary into cohesive contexts, defining each context's core purpose, key aggregates, and context-specific language variations. This phase establishes the high-level architectural boundaries that will guide detailed design decisions.

\subsubsection{Phase 4: Aggregate Design}
Within each identified bounded context, this phase focuses on detailed structural design ensuring proper encapsulation and consistency management (see Appendix~\ref{app:aggregate-design-prompt}). The prompt guides the LLM through identification of aggregate roots, definition of consistency boundaries, specification of contained entities and value objects, and articulation of business invariants. This phase translates conceptual boundaries into concrete structural components.

\subsubsection{Phase 5: Technical Architecture Mapping}
The technical architecture phase translates domain insights into implementable architectural patterns following hexagonal architecture principles (see Appendix~\ref{app:technical-architecture-prompt}). This phase ensures clean separation between domain logic and technical infrastructure while maintaining traceability to business requirements.

\subsubsection{Workflow Integration and Dependencies}
Each phase explicitly builds upon the outputs of the previous ones, creating a cohesive analytical progression from vocabulary definition to detailed implementation guidance. Importantly, each phase is approached as an iterative dialogue with the LLM, allowing for continuous refinement through interactive questioning and clarification until a satisfactory result is achieved. This structured yet flexible process helps prevent common issues such as premature technical decisions or incomplete domain understanding, while ensuring thorough coverage of all relevant DDD architectural concerns.

\section{Requirements Gathering and Preparation}

\subsection{Source Documentation Analysis}
Requirements for both SecuRooms and SecuMails domains were systematically extracted from FTAPI's existing product documentation. This approach ensured that the LLM analysis would be based on the same foundational information used in the development processes.

\subsection{Input Preparation}
For both domains, all gathered requirements were consolidated into structured text documents optimized for LLM processing. Requirements were formatted as plain text documents, with each domain's requirements organized in a single file ready for direct input into the AI system. This approach enabled seamless progression through the five-phase analysis workflow while allowing the LLM to maintain context across all phases and build progressively more detailed architectural insights

\section{Architecture Generation Process}
With the prompt engineering framework established and the requirements documents prepared, the core experimental phase of generating the domain model architectures was executed. This process involved systematically applying the structured five-phase workflow to the requirements for both the SecuRooms and SecuMails domains. The objective was to produce comprehensive, LLM-generated architectural candidates that could subsequently be subjected to rigorous expert evaluation.

\subsection{Execution of the Five-Phase Workflow}
For each domain, the corresponding requirements document was provided as the initial context to the LLM, which was configured with the "Senior DDD Specialist" persona. The model was then guided sequentially through the five predefined phases.

The output from each phase served as the direct input and context for the subsequent phase. This created a cohesive and traceable analytical progression, ensuring that high-level business concepts identified in the early phases directly informed the concrete structural designs in the later phases.

\subsection{Iterative Refinement and Dialogue}
The generation process was not treated as a single, fire-and-forget execution but as an interactive dialogue, consistent with the 'architectural sparring partner' persona. After each phase, the LLMs output was reviewed for clarity, consistency, and completeness. Where ambiguities, logical gaps, or superficial analysis were identified, clarifying follow-up prompts were used to challenge and refine the model's output. This iterative loop was repeated until a satisfactory level of detail and coherence was achieved for a given phase before proceeding to the next. This approach ensured the final architecture was the product of a refined, multi-step analysis rather than a single, unverified generation.

\subsection{Consolidation of Architectural Candidates}
Upon completion of the refined five-phase workflow for each domain, the outputs from all phases were systematically captured and consolidated into a single, structured document. Each document represented a comprehensive architectural candidate, including:
\begin{itemize}
    \item The final Ubiquitous Language glossary.
    \item A map of identified domain events, commands, and actors.
    \item A complete bounded context map with defined responsibilities.
    \item Detailed aggregate designs for each context.
    \item High-level technical architecture mappings based on hexagonal principles.
\end{itemize}
This resulted in two final artifacts: one architectural model for SecuRooms, which could be directly compared against its existing human-designed counterpart, and one for SecuMails, representing a novel modernization proposal. These two candidates formed the basis for the expert evaluation phase.

\subsection{Expert Evaluation Execution}
Following the prepared design, a series of semi-structured interviews were conducted with 3 experts from FTAPI Software GmbH. The participants included one tech lead and two senior developers, each with extensive experience in the company's domain and the principles of Domain-Driven Design. The interviews were conducted via video conference, lasted approximately 50 minutes each, and were recorded and transcribed for analysis. The gathered qualitative data was then thematically analyzed to identify recurring patterns, points of consensus, and divergent opinions regarding the LLM-generated architectural models.